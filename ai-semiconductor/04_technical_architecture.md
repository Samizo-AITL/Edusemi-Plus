# 第4章：AI半導体の技術概要

## 4.1 AI処理の技術的特徴

AI処理、特に深層学習では、以下のような計算負荷が発生します：

- 大規模な行列・テンソル演算（GEMM、Convなど）
- 訓練（Training）では誤差逆伝播や勾配更新
- 推論（Inference）では低レイテンシ処理と省電力性が重要

これらを効率良く処理するため、専用アーキテクチャの開発が進められてきました。

---

## 4.2 主なアーキテクチャの分類と特徴

### GPU（Graphics Processing Unit）
- **開発背景**：元々はグラフィックス処理向け
- **特徴**：膨大な並列スレッドによる高速行列演算  
- **用途**：学習・推論の両方で使用。開発ツールが豊富（CUDA, cuDNN など）

### TPU（Tensor Processing Unit, Google）
- **特徴**：行列積演算に特化したハードウェア（MACアレイ、Bfloat16対応など）  
- **用途**：Google Cloud におけるトレーニングと推論  
- **設計思想**：高密度・低レイテンシ処理、ソフトウェア共設計

### NPU（Neural Processing Unit）
- **特徴**：スマホやIoT端末などエッジ向けAI専用プロセッサ  
- **用途**：画像認識、音声処理、ジェスチャー認識など  
- **設計の焦点**：低電力、高リアルタイム性、SoCとの親和性

### ASIC（Application Specific Integrated Circuit）
- **特徴**：用途特化のため極限まで最適化可能  
- **用途**：企業や研究用途での独自設計（例：Cerebras、Groqなど）  
- **課題**：柔軟性が低く、開発コスト・期間が高い

---

## 4.3 LLM（大規模言語モデル）とハードウェア要件

### LLMの計算要求

- 数十億〜数兆パラメータにおよぶモデル  
- 巨大なトークン系列の処理（Attention演算がボトルネック）  
- 高いメモリ帯域、オンチップメモリ、複雑なデータフロー制御が必要

### ハードウェア設計上のポイント

- **行列演算ユニット（MAC）** の最適化と並列構成  
- **メモリ階層の工夫**（HBM, SRAM, チップレット技術など）  
- **インターコネクトの低レイテンシ化**（NVLink, Infinity Fabric 等）  
- **電力効率**：特にエッジAIやL4クラスの推論で重要

---

## 4.4 ソフトウェアスタックとの協調設計

- **コンパイラ／フレームワーク最適化**（TensorFlow, PyTorch, ONNX）  
- **中間表現（IR）とカーネル生成**の効率  
- **EDAとの統合開発環境**（EDA＋MLのハイブリッド設計）  
- モデルごとの**チューニングパラメータ**を活かす柔軟性も重要

---

AIハードウェアは、汎用プロセッサからの脱却と、  
AIモデルと一体化した「共設計」の時代へと進化しています。
